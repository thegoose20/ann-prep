{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data for Final Annotation\n",
    "Harvesting, transforming, and exporting metadata descriptions for annotation of gendered language in [brat](brat.nlplab.org/).\n",
    "\n",
    "The text in this Jupyter Notebook is organized for uploading into [brat](https://brat.nlplab.org/index.html), where the text will be annotated for instances of gender bias.  The aim of the annotation is to create a gold standard dataset on which a classifier can be trained to identify gender bias in archival metadata descriptions.  \n",
    "\n",
    "This project is focused on the English language and archival institutions in the United Kingdom.\n",
    "\n",
    "* Creator: Lucy Havens\n",
    "* Date: February 2021 - April 2021 (initial harvesting for annotation and training data); April 2023 (harvesting latest catalogue data for automated annotation with classifiers)\n",
    "* Project: PhD research at the School of Informatics, University of Edinburgh\n",
    "* Data Source: Centre for Research Collections' (CRC) [online archival catalog](https://archives.collections.ed.ac.uk/)\n",
    "\n",
    "***\n",
    "**Table of Contents**\n",
    "\n",
    "  [I. Harvesting](#harvesting)\n",
    "\n",
    "  [II. Transforming](#transforming)\n",
    "\n",
    "  [III. Preparing](#preparing)\n",
    "\n",
    "  [IV. Renaming Pre-annotated Files](#renaming)\n",
    "  \n",
    "  [V. Splitting Files Among Annotators](#splitting)\n",
    "  \n",
    "  ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"harvesting\"></a>\n",
    "## I. Harvesting\n",
    "Obtain metadata from the CRC's online archival catalog using the Open Archives Initiative - Protocol for Metadata Harvesting (OAI-PMH).  The CRC provides its metadata in Encoded Archival Description (EAD) format as XML data.  Harvest metadata descriptions from the following metadata fields in the Centre for Research Collections' online catalog:\n",
    "  * Scope and Contents\n",
    "  * Biographical Historical\n",
    "  * Processing Information\n",
    "  * Title\n",
    "  * Language of Material\n",
    "  * Geography Name\n",
    "  * Unit ID\n",
    "  * Encoded Archival Description Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for harvesting\n",
    "import xml.dom.minidom\n",
    "import urllib.request\n",
    "import urllib\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element {http://www.openarchives.org/OAI/2.0/}OAI-PMH at 0x7f7063f57d40>\n"
     ]
    }
   ],
   "source": [
    "archiveMetadataUrl = \"https://aspaceoai.collections.ed.ac.uk/?verb=ListRecords&metadataPrefix=oai_ead\"  #Outdated URL: \"http://lac-archives-live.is.ed.ac.uk:8082/?verb=ListRecords&metadataPrefix=oai_ead\"\n",
    "\n",
    "def getRootFromUrl(url):\n",
    "    content = urllib.request.urlopen(url)\n",
    "\n",
    "    #tree = ET.parse(content)\n",
    "    parser = etree.XMLParser(recover=True)  # Use recover to try to fix broken XML\n",
    "    tree = etree.parse(content, parser)\n",
    "    \n",
    "    root = tree.getroot()\n",
    "    return root\n",
    "\n",
    "root = getRootFromUrl(archiveMetadataUrl)\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: part of or the entirety of a tag name below which you want to get text \n",
    "# Output: a list of text between tags contained within the inputted tagName, \n",
    "#         with one list element per tagName instance\n",
    "def getTextBeneathTag(root, tagName):\n",
    "    text_list = []\n",
    "    for child in root.iter():\n",
    "        tag = child.tag\n",
    "        if tagName in tag:\n",
    "            text_elem = \"\"\n",
    "            for subchild_text in child.itertext():\n",
    "                text_elem = text_elem + subchild_text\n",
    "            text_list.append(text_elem)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: binary value, url for harvesting metadata, starting prefix for the end of the url, and lists of metadata fields to gather\n",
    "# Output: lists of strings of the gathered metadata fields' descriptions, with one string per fonds, series, and item in the catalog\n",
    "def getDescriptiveMetadata(more, archiveMetadataUrlShort, startingPrefix, eadid, ut, ui, ud, gn, lm, sc, bh, pi):    \n",
    "   \n",
    "    archiveMetadataUrlWithPrefix = archiveMetadataUrlShort + startingPrefix\n",
    "    root = getRootFromUrl(archiveMetadataUrlWithPrefix)\n",
    "    eadid.append(getTextBeneathTag(root, \"eadid\"))\n",
    "    ut.append(getTextBeneathTag(root, \"unittitle\"))\n",
    "    ui.append(getTextBeneathTag(root, \"unitid\"))\n",
    "    ud.append(getTextBeneathTag(root, \"unitdate\"))\n",
    "    gn.append(getTextBeneathTag(root, \"geogname\"))\n",
    "    lm.append(getTextBeneathTag(root, \"langmaterial\"))\n",
    "    sc.append(getTextBeneathTag(root, \"scopecontent\"))\n",
    "    bh.append(getTextBeneathTag(root, \"bioghist\"))\n",
    "    pi.append(getTextBeneathTag(root, \"processinfo\"))\n",
    "    resumptionToken = getTextBeneathTag(root, \"resumptionToken\")\n",
    "    \n",
    "    if len(resumptionToken) == 0:\n",
    "        more = False\n",
    "    i = 1\n",
    "    \n",
    "    while more:\n",
    "        archiveMetadataUrlWithToken = archiveMetadataUrlShort + \"resumptionToken=\" + resumptionToken[0]\n",
    "        root = getRootFromUrl(archiveMetadataUrlWithToken)\n",
    "        eadid.append(getTextBeneathTag(root, \"eadid\"))\n",
    "        ut.append(getTextBeneathTag(root, \"unittitle\"))\n",
    "        ui.append(getTextBeneathTag(root, \"unitid\"))\n",
    "        ud.append(getTextBeneathTag(root, \"unitdate\"))\n",
    "        gn.append(getTextBeneathTag(root, \"geogname\"))\n",
    "        lm.append(getTextBeneathTag(root, \"langmaterial\"))\n",
    "        sc.append(getTextBeneathTag(root, \"scopecontent\"))\n",
    "        bh.append(getTextBeneathTag(root, \"bioghist\"))\n",
    "        pi.append(getTextBeneathTag(root, \"processinfo\"))\n",
    "        resumptionToken = getTextBeneathTag(root, \"resumptionToken\")\n",
    "        if len(resumptionToken) == 0:\n",
    "            more = False\n",
    "        i += 1\n",
    "    \n",
    "    print(str(i) + \" resumption tokens\")\n",
    "    return eadid, ut, ui, ud, gn, lm, sc, bh, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663 resumption tokens\n"
     ]
    }
   ],
   "source": [
    "url = \"https://aspaceoai.collections.ed.ac.uk/?verb=ListRecords&\"   #Outdated URL: \"http://lac-archives-live.is.ed.ac.uk:8082/?verb=ListRecords&\"\n",
    "startPrefix = \"metadataPrefix=oai_ead\"\n",
    "eadid = [] # List of fonds-level identifiers\n",
    "ut = [] # List of fonds, series, and item titles\n",
    "ui = [] # List of fonds, series, and item identifiers\n",
    "ud = [] # List of fonds, series, and item dates\n",
    "gn = [] # List of fonds, series, and item associated geographic locations \n",
    "lm = [] # List of fonds, series, and item material languages\n",
    "sc = [] # List of fonds, series, and item \"Scope and Contents\" descriptions\n",
    "bh = [] # List of fonds, series, and item \"Biographical / Historical\" descriptions\n",
    "pi = []  # List of fonds, series, and item \"Processing Information\" descriptions\n",
    "\n",
    "eadid, ut, ui, ud, gn, lm, sc, bh, pi = getDescriptiveMetadata(True, url, startPrefix, eadid, ut, ui, ud, gn, lm, sc, bh, pi)  # initial number of resumption tokens: 1081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(eadid) == len(ut)\n",
    "assert len(ut) == len(ui)\n",
    "assert len(ui) == len(ud)\n",
    "assert len(gn) == len(ui)\n",
    "assert len(lm) == len(sc)\n",
    "assert len(eadid) == len(sc)\n",
    "assert len(bh) == len(pi)\n",
    "assert len(pi) == len(eadid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "124\n",
      "124\n",
      "124\n",
      "116\n",
      "0\n",
      "119\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(len(eadid[i]))  # 1\n",
    "print(len(ut[i]))     # 124\n",
    "print(len(ui[i]))     # 124\n",
    "print(len(ud[i]))     # 124\n",
    "print(len(gn[i]))     # 116\n",
    "print(len(lm[i]))     # 125\n",
    "print(len(sc[i]))     # 119\n",
    "print(len(bh[i]))     # 2\n",
    "print(len(pi[i]))     # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def deduplicateDescriptions(metadata_field_list):\n",
    "#     unique_descs = []\n",
    "#     for fonds in metadata_field_list:\n",
    "#         unique_descs += [list(set(fonds))]\n",
    "#     assert len(metadata_field_list) == len(unique_descs)\n",
    "#     return unique_descs\n",
    "# unique_sc = deduplicateDescriptions(sc)\n",
    "# unique_bh = deduplicateDescriptions(bh)\n",
    "# unique_pi = deduplicateDescriptions(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"transforming\"></a>\n",
    "## II. Transforming\n",
    "Create a table (pandas DataFrame) of the metadata without multi-sentence descriptions and plain text files of the descriptive metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Coll-1064'], ['Coll-31'], ['Coll-51'], ['Coll-204'], ['Coll-206'], ['Coll 205'], ['Coll-1443'], ['Coll-1444'], ['Coll-1391'], ['Coll-1371']]\n"
     ]
    }
   ],
   "source": [
    "print(eadid[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = []\n",
    "for sublist in eadid:\n",
    "    for item in sublist:\n",
    "        flatten += [item]\n",
    "assert type(flatten[0]) == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(eadid) == len(flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eadid</th>\n",
       "      <th>unit_title</th>\n",
       "      <th>unit_identifier</th>\n",
       "      <th>unit_date</th>\n",
       "      <th>geography</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coll-1064</td>\n",
       "      <td>[Papers of Professor Walter Ledermann, 1 (37),...</td>\n",
       "      <td>[Coll-1064, Coll-1064/1, Coll-1064/2, Coll-106...</td>\n",
       "      <td>[1937-1954, 2 Feb 1937, 10 Feb 1937, 16 Feb 19...</td>\n",
       "      <td>[Edinburgh -- Scotland, Edinburgh -- Scotland,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coll-31</td>\n",
       "      <td>[Drawings from the Office of Sir Rowand Anders...</td>\n",
       "      <td>[Coll-31, Coll-31/1, Coll-31/1/1, Coll-31/1/1/...</td>\n",
       "      <td>[1814-1924, 1874-1905, 1874-1879, 1874-1875, 1...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coll-51</td>\n",
       "      <td>[Papers of Sir Roderick Impey Murchison and hi...</td>\n",
       "      <td>[Coll-51, Coll-51/1, Coll-51/2, Coll-51/2/1, C...</td>\n",
       "      <td>[1771-1935, 1723-1935, 1770-1938, 1770-1938, 1...</td>\n",
       "      <td>[Calcutta (India), Europe, Scotland, Tarradale...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coll-204</td>\n",
       "      <td>[Lecture Notes of John Robison, Introductions,...</td>\n",
       "      <td>[Coll-204, Coll-204/1, Coll-204/2, Coll-204/3,...</td>\n",
       "      <td>[c1779-c1801, c1779-c1801, c1804, c1802, c1780...</td>\n",
       "      <td>[Edinburgh -- Scotland, Glasgow Lanarkshire Sc...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coll-206</td>\n",
       "      <td>[Records of the Wernerian Natural History Soci...</td>\n",
       "      <td>[Coll-206, Coll-206/1, Coll-206/1/1, Coll-206/...</td>\n",
       "      <td>[1808-1858, 12 January 1808-16 April 1858, 12 ...</td>\n",
       "      <td>[Edinburgh -- Scotland, Freiburg im Breisgau (...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       eadid                                         unit_title  \\\n",
       "0  Coll-1064  [Papers of Professor Walter Ledermann, 1 (37),...   \n",
       "1    Coll-31  [Drawings from the Office of Sir Rowand Anders...   \n",
       "2    Coll-51  [Papers of Sir Roderick Impey Murchison and hi...   \n",
       "3   Coll-204  [Lecture Notes of John Robison, Introductions,...   \n",
       "4   Coll-206  [Records of the Wernerian Natural History Soci...   \n",
       "\n",
       "                                     unit_identifier  \\\n",
       "0  [Coll-1064, Coll-1064/1, Coll-1064/2, Coll-106...   \n",
       "1  [Coll-31, Coll-31/1, Coll-31/1/1, Coll-31/1/1/...   \n",
       "2  [Coll-51, Coll-51/1, Coll-51/2, Coll-51/2/1, C...   \n",
       "3  [Coll-204, Coll-204/1, Coll-204/2, Coll-204/3,...   \n",
       "4  [Coll-206, Coll-206/1, Coll-206/1/1, Coll-206/...   \n",
       "\n",
       "                                           unit_date  \\\n",
       "0  [1937-1954, 2 Feb 1937, 10 Feb 1937, 16 Feb 19...   \n",
       "1  [1814-1924, 1874-1905, 1874-1879, 1874-1875, 1...   \n",
       "2  [1771-1935, 1723-1935, 1770-1938, 1770-1938, 1...   \n",
       "3  [c1779-c1801, c1779-c1801, c1804, c1802, c1780...   \n",
       "4  [1808-1858, 12 January 1808-16 April 1858, 12 ...   \n",
       "\n",
       "                                           geography language  \n",
       "0  [Edinburgh -- Scotland, Edinburgh -- Scotland,...       []  \n",
       "1                                                 []       []  \n",
       "2  [Calcutta (India), Europe, Scotland, Tarradale...       []  \n",
       "3  [Edinburgh -- Scotland, Glasgow Lanarkshire Sc...       []  \n",
       "4  [Edinburgh -- Scotland, Freiburg im Breisgau (...       []  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta = pd.DataFrame.from_dict({\"eadid\":flatten,\"unit_title\":ut, \"unit_identifier\":ui, \"unit_date\":ud, \"geography\":gn, \"language\":lm})\n",
    "# df = pd.read_csv(\"CRC_units-grouped-by-fonds.csv\")\n",
    "# df_meta = df_meta.set_index(\"eadid\")\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(663, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df_meta.shape)\n",
    "df_meta.to_csv(\"CRC_units-grouped-by-fonds_April2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', 'BAI', 'Coll 205', 'Coll-100', 'Coll-1000']\n"
     ]
    }
   ],
   "source": [
    "ids = list(df_meta[\"eadid\"])\n",
    "ids.sort()\n",
    "print(ids[:10])  # 6 of these are empty strings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give EADIDs that are empty strings a name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663\n",
      "['Coll-1064', 'Coll-31', 'Coll-51', 'Coll-204', 'Coll-206', 'Coll 205', 'Coll-1443', 'Coll-1444', 'Coll-1391', 'Coll-1371']\n"
     ]
    }
   ],
   "source": [
    "indeces = []\n",
    "no_ids = 0\n",
    "for ui in flatten:\n",
    "    if ui == \"\":\n",
    "        indeces += [\"no_id\"+str(no_ids)]\n",
    "        no_ids += 1\n",
    "    else:\n",
    "        indeces += [ui]\n",
    "print(len(indeces))\n",
    "print(indeces[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def flattenTwoDimensionalList(two_d_list):\n",
    "#     flattened = []\n",
    "#     for listoflists in two_d_list:\n",
    "#         for unit in listoflists:\n",
    "#             flattened += [unit]\n",
    "#     return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# titles = flattenTwoDimensionalList(ut)\n",
    "# # print(titles[0:30])\n",
    "# identifiers = flattenTwoDimensionalList(ui)\n",
    "# dates = flattenTwoDimensionalList(ud)\n",
    "# geogs = flattenTwoDimensionalList(gn)\n",
    "# lang = flattenTwoDimensionalList(lm)\n",
    "# scopecont = flattenTwoDimensionalList(sc)\n",
    "# bioghist = flattenTwoDimensionalList(bh)\n",
    "# procinfo = flattenTwoDimensionalList(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print(len(titles))\n",
    "# print(len(identifiers))\n",
    "# print(len(dates))\n",
    "# print(len(geogs))\n",
    "# print(len(lang))\n",
    "# print(len(scopecont))\n",
    "# print(len(bioghist))\n",
    "# print(len(procinfo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dir_name = \"descriptions_April2023/\"   #\"descriptions_by_fonds/\"\n",
    "Path(dir_name).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeListsToFilesPerFonds(indeces, titles, scopeconts, bioghists, procinfo):\n",
    "    maxI = len(indeces)\n",
    "    i = 0\n",
    "    while i < maxI:\n",
    "        filename = (indeces[i]).strip()\n",
    "        filename = filename.replace(\" \", \"_\")\n",
    "        filename = filename.replace(\"/\", \"_\")\n",
    "        filepath = dir_name+filename+\".txt\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(\"Identifier: \")\n",
    "            f.write(filename + \"\\n\")\n",
    "            for t in titles[i]:\n",
    "                t = t.strip()\n",
    "                f.write(\"\\nTitle:\\n\")\n",
    "                f.write(t + \"\\n\")\n",
    "            for s in scopeconts[i]:\n",
    "                s = s.strip()\n",
    "                f.write(\"\\nScope and Contents:\\n\")\n",
    "                f.write(s + \"\\n\")\n",
    "            for b in bioghists[i]:\n",
    "                b = b.strip()\n",
    "                f.write(\"\\nBiographical / Historical:\\n\")\n",
    "                f.write(b + \"\\n\")\n",
    "            for p in procinfo[i]:\n",
    "                p = p.strip()\n",
    "                f.write(\"\\nProcessing Information:\\n\")\n",
    "                f.write(p + \"\\n\")\n",
    "        f.close()\n",
    "        i += 1\n",
    "    return str(maxI) + \" files written\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'663 files written'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writeListsToFilesPerFonds(indeces, ut, sc, bh, pi)  #unique_sc, unique_bh, unique_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preparing\"></a>\n",
    "## III. Preparing\n",
    "Prepare the files for annotation, ensuring ease in reading and splitting any excessively long files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Libraries for Natural Language Processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.text import Text\n",
    "# nltk.download('punkt')\n",
    "# from nltk.probability import FreqDist\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = PlaintextCorpusReader(dir_name, '.+')\n",
    "tokens = files.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Identifier', ':', 'BAI', 'Title', ':', 'Papers', 'of', 'Professor', 'John', 'Baillie', ',', 'and', 'Baillie', 'Family', 'Papers', 'Title', ':', 'Papers', 'of', 'John']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BAI.txt', 'Coll-100.txt', 'Coll-1000.txt', 'Coll-1001.txt', 'Coll-1004.txt', 'Coll-1010.txt', 'Coll-1014.txt', 'Coll-1018.txt', 'Coll-1021.txt', 'Coll-1024.txt']\n"
     ]
    }
   ],
   "source": [
    "token_totals = []\n",
    "filenames = files.fileids()\n",
    "for f in filenames:\n",
    "        token_totals += [len(files.words(f))]\n",
    "file_lengths = dict(zip(filenames,token_totals))\n",
    "print(filenames[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8, 9, 14, 2063, 15, 24595, 4115, 2068, 19, 23, 21, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 12333, 48, 47, 50, 51, 10290, 8242, 54, 55, 56, 24632, 52, 59, 57, 61, 58, 63, 64, 65, 66, 67, 68, 60, 70, 71, 72, 73, 74, 75, 8269, 78, 79, 77, 80, 82, 83, 81, 85, 86, 89, 91, 95, 97, 99, 100, 103, 104, 105, 106, 107, 108, 109, 110, 114, 116, 118, 119, 120, 121, 24, 124, 125, 126, 127, 128, 129, 131, 2180, 133, 134, 136, 137, 138, 139, 140, 142, 143, 144, 145, 147, 148, 149, 150, 151, 153, 154, 156, 157, 158, 159, 161, 163, 164, 45221, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 2223, 184, 186, 187, 185, 190, 192, 196, 198, 199, 200, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 222, 223, 224, 225, 226, 227, 230, 231, 233, 234, 235, 28908, 236, 237, 28911, 240, 241, 242, 239, 244, 245, 243, 247, 248, 249, 250, 251, 252, 253, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 53, 272, 273, 274, 275, 276, 277, 279, 280, 282, 283, 284, 286, 287, 288, 289, 290, 291, 292, 294, 295, 2343, 297, 299, 300, 301, 302, 303, 304, 305, 307, 308, 309, 311, 312, 313, 314, 62, 316, 317, 318, 319, 321, 323, 324, 327, 328, 329, 330, 331, 332, 333, 336, 337, 338, 339, 12626, 341, 342, 343, 340, 344, 346, 347, 345, 349, 350, 351, 352, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 369, 371, 372, 374, 375, 377, 379, 381, 383, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 399, 12689, 402, 403, 405, 406, 407, 409, 410, 411, 413, 415, 417, 419, 420, 421, 423, 427, 428, 429, 430, 431, 432, 433, 434, 435, 437, 438, 439, 440, 441, 442, 444, 446, 448, 449, 450, 4547, 456, 458, 462, 464, 465, 467, 470, 471, 472, 473, 474, 475, 476, 477, 478, 480, 486, 10728, 490, 491, 492, 493, 494, 495, 497, 498, 499, 500, 502, 503, 505, 507, 508, 510, 512, 513, 8706, 516, 389638, 519, 521, 523, 524, 525, 526, 527, 530, 531, 533, 536, 542, 543, 546, 547, 548, 554, 555, 557, 558, 560, 563, 567, 568, 569, 43579, 572, 574, 576, 578, 583, 584, 585, 608842, 587, 586, 590, 593, 595, 598, 600, 605, 607, 80479, 609, 613, 614, 618, 619, 624, 628, 629, 634, 635, 636, 637, 642, 643, 648, 651, 653, 654, 656, 658, 659, 660, 661, 666, 671, 675, 676, 677, 678, 680, 681, 682, 684, 689, 690, 8883, 696, 698, 146116, 709, 712, 713, 6859, 716, 724, 725, 733, 13027, 740, 742, 744, 756, 759, 770, 2820, 774, 779, 780, 787, 6934, 803, 17199, 815, 821, 826, 828, 832, 833, 835, 837, 838, 839, 851, 859, 861, 9059, 870, 876, 877, 4980, 889, 2942, 895, 35716, 7051, 916, 918, 926, 927, 2978, 930, 939, 947, 964, 11226, 3042, 1000, 1006, 1008, 1010, 1022, 1035, 1040, 1041, 1054, 7198, 1059, 3110, 1064, 1066, 1067, 1072, 3122, 1076, 3137, 1092, 23639, 1112, 11359, 3175, 1131, 3200, 1153, 1156, 1168, 1175, 1180, 1183, 1186, 1190, 1191, 1196, 1209, 11451, 1223, 1242, 3302, 1303, 1309, 5417, 1341, 3415, 1367, 1393, 1408, 3462, 1415, 1482, 1485, 1488, 1501, 7648, 1505, 1519, 1527, 120325, 3594, 1559, 97822, 1603, 1605, 28230, 1632, 20066, 13930, 1654, 212647, 32424, 1706, 1721, 7875, 32480, 3829, 1786, 16139, 5966, 1947, 63458}\n"
     ]
    }
   ],
   "source": [
    "token_totals.sort\n",
    "print(set(token_totals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "826"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_lengths[\"Coll-1250.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "too_long = []\n",
    "for key,value in file_lengths.items():\n",
    "    if value > 1000:\n",
    "        too_long += [key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BAI.txt', 'Coll-1022.txt', 'Coll-1036.txt', 'Coll-1052.txt', 'Coll-1057.txt', 'Coll-1059.txt', 'Coll-1060.txt', 'Coll-1061.txt', 'Coll-1062.txt', 'Coll-1064.txt', 'Coll-1066.txt', 'Coll-1142.txt', 'Coll-1146.txt', 'Coll-1156.txt', 'Coll-1162.txt', 'Coll-1167.txt', 'Coll-1242.txt', 'Coll-1243.txt', 'Coll-1247.txt', 'Coll-1255.txt', 'Coll-1257.txt', 'Coll-1260.txt', 'Coll-1266.txt', 'Coll-1294.txt', 'Coll-13.txt', 'Coll-1310.txt', 'Coll-1320.txt', 'Coll-1329.txt', 'Coll-1357.txt', 'Coll-1362.txt', 'Coll-1363.txt', 'Coll-1364.txt', 'Coll-1373.txt', 'Coll-1383.txt', 'Coll-1385.txt', 'Coll-14.txt', 'Coll-1434.txt', 'Coll-1443.txt', 'Coll-146.txt', 'Coll-1461.txt', 'Coll-1489.txt', 'Coll-1490.txt', 'Coll-1492.txt', 'Coll-1496.txt', 'Coll-1497.txt', 'Coll-1499.txt', 'Coll-1527.txt', 'Coll-1528.txt', 'Coll-1541.txt', 'Coll-1549.txt', 'Coll-1557.txt', 'Coll-1574.txt', 'Coll-1577.txt', 'Coll-1580.txt', 'Coll-1583.txt', 'Coll-1586.txt', 'Coll-1593.txt', 'Coll-16.txt', 'Coll-1613.txt', 'Coll-1623.txt', 'Coll-1638.txt', 'Coll-164.txt', 'Coll-1644.txt', 'Coll-1650.txt', 'Coll-1651.txt', 'Coll-1657.txt', 'Coll-1671.txt', 'Coll-1690.txt', 'Coll-17.txt', 'Coll-1700.txt', 'Coll-1711.txt', 'Coll-1715.txt', 'Coll-1725.txt', 'Coll-1728.txt', 'Coll-1749.txt', 'Coll-1756.txt', 'Coll-1758.txt', 'Coll-1798.txt', 'Coll-203.txt', 'Coll-204.txt', 'Coll-205.txt', 'Coll-206.txt', 'Coll-21.txt', 'Coll-27.txt', 'Coll-31.txt', 'Coll-36.txt', 'Coll-39.txt', 'Coll-41.txt', 'Coll-411.txt', 'Coll-42.txt', 'Coll-43.txt', 'Coll-51.txt', 'Coll-54.txt', 'Coll-64.txt', 'Coll-66.txt', 'Coll-67.txt', 'Coll-68.txt', 'Coll-70.txt', 'Coll-706.txt', 'Coll-71.txt', 'Coll-74.txt', 'Coll-85.txt', 'Coll-99.txt', 'ECA_TYP.txt', 'EUA_CA1.txt', 'EUA_CA14.txt', 'EUA_CA2.txt', 'EUA_GD31.txt', 'EUA_GD46.txt', 'EUA_GD58.txt', 'EUA_GD9.txt', 'EUA_IN1.txt', 'EUA_IN2.txt', 'EUA_IN20.txt', 'EUA_IN23.txt', 'GD2.txt', 'La.txt', 'Or_Ms.txt', 'PJM.txt']\n",
      "119\n"
     ]
    }
   ],
   "source": [
    "print(too_long)\n",
    "print(len(too_long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of files to break up manually, so let's use Python to divide these large files into smaller files with a maximum of 100 lines each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code in this cell based on:\n",
    "# https://stackoverflow.com/questions/16289859/splitting-large-text-file-into-smaller-text-files-by-line-numbers-using-python\n",
    "def splitLargeFile(f, max_lines, old_dir, new_dir):\n",
    "    short = None\n",
    "    file_path = old_dir+f\n",
    "    with open(file_path) as long:\n",
    "        for line_no, line in enumerate(long):\n",
    "            if line_no % max_lines == 0:\n",
    "                if short:\n",
    "                    short.close()\n",
    "                f = f.replace(\".txt\",\"_\")\n",
    "                short_name = str(f)+'{}.txt'.format(line_no + max_lines)\n",
    "                new_path = new_dir+short_name\n",
    "                short = open(new_path, \"w\")\n",
    "            short.write(line)\n",
    "        if short:\n",
    "            short.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_directory = 'descriptions_split_April2023/'\n",
    "Path(split_directory).mkdir(parents=True, exist_ok=True)\n",
    "for f in filenames:\n",
    "    splitLargeFile(f, 100, dir_name, split_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 2377\n"
     ]
    }
   ],
   "source": [
    "# directory = 'descriptions_by_fonds_split/'\n",
    "files = PlaintextCorpusReader(split_directory, '.+')\n",
    "print(\"Total files:\",len(files.fileids()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"renaming\"></a>\n",
    "## IV. Renaming Pre-Annotated Files\n",
    "Renaming so the files are properly ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"descriptions_by_fonds_split\"\n",
    "filenames = os.listdir(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "max_digits = 0\n",
    "for f in filenames:\n",
    "    fend = re.findall(\"\\d+\\.ann|\\d+\\.txt\",f)\n",
    "    if fend:\n",
    "        fdigits = len(re.findall(\"\\d\",fend[0])) \n",
    "        if fdigits > max_digits:\n",
    "            max_digits = fdigits\n",
    "print(max_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad file names with zeros so all are 5 digits long\n",
    "for f in filenames:\n",
    "    oldpath = os.path.join(datadir,f)\n",
    "    end_list = re.findall(\"\\d+\\.ann|\\d+\\.txt\",f)\n",
    "    if len(end_list) > 0:\n",
    "        start = f.replace(end_list[0],\"\")\n",
    "        digits = len(re.findall(\"\\d\",end_list[0]))\n",
    "        new_f = start + \"0\" * (max_digits - digits) + end_list[0]\n",
    "        newpath = os.path.join(datadir,new_f)\n",
    "        os.rename(oldpath, newpath)\n",
    "        \n",
    "# Note: code in this cell based on: https://stackoverflow.com/questions/2491222/how-to-rename-a-file-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Put files in subfolders based on fonds (collection) identifier\n",
    "# filenames = os.listdir(datadir)\n",
    "# for f in filenames:\n",
    "#     oldpath = os.path.join(datadir,f)\n",
    "#     end_list = re.findall(\"\\d+\\.ann|\\d+\\.txt\",f)\n",
    "#     if len(end_list) > 0:\n",
    "#         identifier = f.replace(end_list[0],\"\")\n",
    "#         new_dir = os.path.join(datadir,identifier)\n",
    "#         try:\n",
    "#             os.makedirs(new_dir)\n",
    "#         except FileExistsError:\n",
    "#             # directory already exists\n",
    "#             pass\n",
    "#         newpath = os.path.join(new_dir,f)\n",
    "#         os.rename(oldpath, newpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"splitting\"></a>\n",
    "## V. Splitting Files Among Annotators\n",
    "* Total annotators: 5\n",
    "    * A1 and A2 to annotate with Person Name and Linguistic labels\n",
    "    * A3 and A4 to annotate with Contextual labels\n",
    "* Inter-annotator agreement (IAA) will be evaluated for:\n",
    "    * A1 and A2\n",
    "    * Me and A1\n",
    "    * Me and A2\n",
    "    * A3 and A4\n",
    "    * Me and A3\n",
    "    * Me and A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated files each annotator will label: 720\n",
      "Estimated files both pairs of annotators will label in total: 1440\n"
     ]
    }
   ],
   "source": [
    "# My pilot with finalized instructions begun 16:05 and ended at 16:35, many of the 15 files I \n",
    "# annotated were short on description, so let's estimate an average of 10 file in an hour, to be safe\n",
    "hired_hours = 9*8  # each annotator working 9 hours per week for 8 weeks\n",
    "est_files_annotated = hired_hours * 10\n",
    "print(\"Estimated files each annotator will label:\", est_files_annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allow 10% overlap for each pair of annotators, totaling 72 files\n",
      "Estimated files both pairs of annotators will label in total: 1368\n"
     ]
    }
   ],
   "source": [
    "print(\"Allow 10% overlap for each pair of annotators, totaling\", str(int(720*0.1)), \"files\")\n",
    "print(\"Estimated files both pairs of annotators will label in total:\", (est_files_annotated * 2) - int(720*0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7298\n",
      "AA4_00100.ann\n",
      "AA4_00100.txt\n",
      "AA5_00100.ann\n",
      "AA5_00100.txt\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "directory = \"../AnnotationData/descriptions_by_fonds_split_with_ann/descriptions_by_fonds_split_with_ann\"\n",
    "descs_split = list(os.listdir(directory))\n",
    "descs_split.sort()\n",
    "descs_split.pop(0)\n",
    "print(len(descs_split))\n",
    "print(descs_split[0])\n",
    "print(descs_split[1])\n",
    "print(descs_split[2])\n",
    "print(descs_split[3])\n",
    "print(\".txt\" in descs_split[1])\n",
    "# print(descs_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n",
      "2848\n"
     ]
    }
   ],
   "source": [
    "print(73*2)\n",
    "print(1461+1460-73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select approximately 10% of the total number of files for the hired annotators, and select approximately 10% of that number of files to be annotated by the all the hired annotators.\n",
    "* 730 txt files per annotator (including ann files, 1460 files total)\n",
    "* First 73 txt files annotated by everyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460\n",
      "1533\n",
      "['Coll-1434_14300.ann', 'Coll-1434_14300.txt', 'Coll-1434_14400.ann', 'Coll-1434_14400.txt', 'Coll-1434_14500.ann', 'Coll-1434_14500.txt', 'Coll-1434_14600.ann', 'Coll-1434_14600.txt', 'Coll-1434_14700.ann', 'Coll-1434_14700.txt']\n",
      "['Coll-1584_00100.ann', 'Coll-1584_00100.txt', 'Coll-1585_00100.ann', 'Coll-1585_00100.txt', 'Coll-1586_00100.ann', 'Coll-1586_00100.txt', 'Coll-1586_00200.ann', 'Coll-1586_00200.txt', 'Coll-1586_00300.ann', 'Coll-1586_00300.txt']\n"
     ]
    }
   ],
   "source": [
    "fonds1 = descs_split[0:1460]                           # 730 * 2 to account for .ann files\n",
    "fonds2 = descs_split[0:146] + descs_split[1461:2848]   # only first 76 txt files (146 with ann files) should overlap\n",
    "        \n",
    "print(len(fonds1))\n",
    "print(len(fonds2))\n",
    "print(fonds1[-10:])\n",
    "print(fonds2[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the select files of metadata descriptions into folders for each annotator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in fonds1:\n",
    "    oldpath = os.path.join(directory,f)\n",
    "    newpath1 = os.path.join(\"ann1\",f)  # paired with ann2\n",
    "    newpath2 = os.path.join(\"ann3\",f)  # paired with ann4\n",
    "    copyfile(oldpath, newpath1)\n",
    "    copyfile(oldpath, newpath2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Update June 10, 2021:** A3 finished annotating allocated files, so select more files to upload to their brat directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonds1B = descs_split[1461:1561]\n",
    "for f in fonds1B:\n",
    "    oldpath = os.path.join(directory,f)\n",
    "    newpath = os.path.join(\"../AnnotationData/ann3B\",f)\n",
    "    copyfile(oldpath, newpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in fonds2:\n",
    "    oldpath = os.path.join(directory,f)\n",
    "    newpath3 = os.path.join(\"ann2\",f)  # paired with ann1\n",
    "    newpath4 = os.path.join(\"ann4\",f)  # paired with ann3\n",
    "    copyfile(oldpath, newpath3)\n",
    "    copyfile(oldpath, newpath4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countWordsInDirectory(directory):\n",
    "    files = PlaintextCorpusReader(directory, '.+\\.txt')\n",
    "    tokens = files.words()\n",
    "    print(str(directory)+\": \" + str(len(tokens)))\n",
    "    return\n",
    "\n",
    "pair1_words = countWordsInDirectory(\"ann1/\")                                   # 486880\n",
    "pair2_words = countWordsInDirectory(\"ann3/\")                                   # 595018\n",
    "total_words = countWordsInDirectory(\"descriptions_by_fonds_split_with_ann\")    # 2754044"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of dataset annotated by annotator pair 1: 17.678729896835343\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of dataset annotated by annotator pair 1:\",(pair1_words/total_words)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of dataset annotated by annotator pair 2: 21.60524668451194\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of dataset annotated by annotator pair 2:\",(pair2_words/total_words)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If each pair of annotators annotates about half the total files allocated to them, in total, about 10% of my entire dataset will be doubly annotated (because I'm labeling all files with all categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89273\n"
     ]
    }
   ],
   "source": [
    "overlap = descs_split[0:146]\n",
    "files = PlaintextCorpusReader(directory, '.+\\.txt')\n",
    "fileids = files.fileids()\n",
    "overlap_token_count = 0\n",
    "for f in fileids:\n",
    "    if f in overlap:\n",
    "        overlap_token_count += len(files.words(f))\n",
    "print(overlap_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2415241005590323\n"
     ]
    }
   ],
   "source": [
    "total_tokens = 2754044\n",
    "print((overlap_token_count/total_tokens)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files that EVERYONE annotates represent about 3% of the total dataset, meaning about 3% of the data will be triply annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
