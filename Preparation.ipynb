{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Metadata Descriptions for Annotation in brat\n",
    "\n",
    "The text in this Jupyter Notebook is organized for uploading into [brat](https://brat.nlplab.org/index.html), where the text will be annotated for instances of gender bias.  The aim of the annotation is to create a gold standard dataset on which a classifier can be trained to identify gender bias in archival metadata descriptions.  \n",
    "\n",
    "This project is focused on the English language and archival institutions in the United Kingdom.\n",
    "\n",
    "* Author: Lucy Havens\n",
    "* Date: November 17, 2020 - TBD\n",
    "* Project: PhD Case Study 1\n",
    "* Data Source: Files of select metadata descriptions extracted and exported in [the GitHub repo, annot-prep](https://github.com/thegoose20/annot-prep)\n",
    "* Data Provider: [ArchivesSpace](https://archives.collections.ed.ac.uk/), Centre for Research Collections, University of Edinburgh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /afs/inf.ed.ac.uk/user/s15/s1545703/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /afs/inf.ed.ac.uk/user/s15/s1545703/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /afs/inf.ed.ac.uk/user/s15/s1545703/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries for data analysis and visualization\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "from collections import Counter\n",
    "\n",
    "# To avoid SSL error when downloading NLTK packages...\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "# nltk.download()\n",
    "\n",
    "# Libraries for Natural Language Processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.text import Text\n",
    "nltk.download('punkt')\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Other useful libraries\n",
    "import string\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "The annotation dataset will be created from TXT files of extracted metadata descriptions designated for training and development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonds ID:Coll-1149\n",
      "unitids\n",
      "{'Coll-1149'}\n",
      "scopecontent\n",
      "{'This key research resource is an important survival, being a manuscript account book detailing transactions - debits and credits - relating to the lead-ore company at Leadhills, operated by Sir John Hope of Craighall. Many important people are mentioned in this book, including Alexander Hope of London, Archibald Hope of Craighall, the Earl of Wigtown, the Duke of Hamilton, the Lord of Inglestone, Charles Erskine of Alba, Alexander Tait, Lady Marie Keith, the Earl of Crawford, Lord Mordington, Lord Cardcross, and Alexander Ross. The amounts involved are huge, with the account of revenues in hand running to over £70,000 towards the end of the period. The manuscript volume itself is composed of a short alphabetic table of names, then from folio 1, accounts dating from 1 August 1662, Edinburgh, to 7 September 1671, Edinburgh, at folio 221. Towards the rear of the volume are another set of accounts and revenues and interests on 87 folios.'}\n",
      "bioghist\n",
      "{\"The Scottish judge, Sir John Hope, Lord Craighall, was born around 1605 and was the eldest son of Sir Thomas Hope of Craighall, first baronet. He studied for the legal profession, became an advocate, and was knighted in 1632. In 1640 he was placed on the Committee of Estates appointed to provide for the defence of the country against King Charles I (the Committee of Estates had been appointed to govern Scotland when Parliament was not in session). In 1651, his brother Sir Alexander Hope was examined by the Committee of Estates for advising the King to surrender Scotland and Ireland to Cromwell, during which he quoted Sir John, Lord Craighall, as having stated that the King should 'treat with Cromwell for one-half of his coat before he lost the whole'. In 1652, Sir John was appointed as one of three Scottish judges who, together with five English judges, comprised Cromwell's committee responsible for the administration of justice. Another brother, Sir James Hope (1614-1661), was a representative for Scotland in the English Parliament in 1653. Sir John Hope, Lord Craighall, died in Edinburgh on 28 April 1654. It had been Sir John Hope and Sir James Hope who in the 1660s had owned the lead-ore company at Leadhills in the Lowther Hills - an area along with Wanlockhead which yielded other ores too. Indeed gold was discovered in the area during the reign of James IV, when gold-mining had provided employment for 300 persons. Although gold-mining declined, the export of lead-ore from Leadhills was very important in the early modern Scottish economy, and the Hope family had built on their control of this industry to acquire property and land. The lead mines were profitable enough to justify building or improving roads the full 50 miles to Leith from which the ore could be exported to the Low Countries for smelting.\"}\n",
      "processinfo\n",
      "set()\n",
      "Fonds ID:Coll-1148\n",
      "unitids\n",
      "{'Coll-1148'}\n",
      "scopecontent\n",
      "{\"The manuscript materialLe Thresor des Divines et Celestes Consolations'(London, 1643) was bound by 'Lord Herbert's Binder'. It contains 21 chapters on the nature and benefits of Afflictions. A rough translation of the introduction gives the flavour: 'Friendly Reader, this book, to which I have given light, shows how tribulations tear us away from sin, which is the source and origin of all pain; it brings us to virtue, to good, and to God, who is the means, the Principle, indeed who is in Himself all the Sovereign good. And afterwards it produces the means to keep always on the right path of virtue, eases our path towards Heaven, and forces us through a secret violence and voluntary constraints, despising that which is of the world (holding its voluptuousness, its delights and vanities against one's will and in disgust) and to breathe towards Heaven, with tears in the eyes, sighs on the lips and sobs in the heart.' The volume is dedicated to Edward Montagu, 2nd. Earl of Manchester (1602-1671), the Presbyterian and a Parliamentary leader. Manchester had succeeded his father as 2nd. Earl of Manchester in November 1642. In August 1643 he was appointed Sergeant-Major-General of the Associated Counties of East Anglia.\"}\n",
      "bioghist\n",
      "{\"The name David de Hasteville, in London, first appears in theCalendar of State Papers, Domestic series, of the reign of Charles I. 1640-41., p.377, in the 'Petition of David de Hasteville, formerly called Father Archange de Hasteville, Abbé du Val de Sainte Croix and General of the Order of St. Romuald to [Sec. Windebank]'. De Hasteville had recently become a Protestant and was seeking 'some sort of entertainment or pension'. He had also petitioned the House of Commons in 1648, describing himself as a Protestant who had recently left his native France and 'a plentiful estate'. Apparently in 1643 he had been appointed general of artillery under Sir William Waller, and had raised three hundred and fourteen men at his own cost, not receiving any government cash, and was then in danger of arrest for money owed for th\n"
     ]
    }
   ],
   "source": [
    "dataset1 = open('DatasetExports/UoEArchivesMetadata_ID-SC-BH-PI_trainingset1.txt', 'r')\n",
    "dataset2 = open('DatasetExports/UoEArchivesMetadata_ID-SC-BH-PI_trainingset2.txt', 'r')\n",
    "dataset3 = open('DatasetExports/UoEArchivesMetadata_ID-SC-BH-PI_trainingset3.txt', 'r')\n",
    "dataset4 = open('DatasetExports/UoEArchivesMetadata_ID-SC-BH-PI_devset.txt', 'r')\n",
    "\n",
    "dataset1 = dataset1.read()\n",
    "dataset2 = dataset2.read()\n",
    "dataset3 = dataset3.read()\n",
    "dataset4 = dataset4.read()\n",
    "\n",
    "print(dataset1[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat the Data\n",
    "Remove extraneous characters and add new lines to make the text more readable, and split the text into one file per collection (\"fonds\" in archive-speak). In brat, annotations will be made on collection descriptions (including descriptions for the collection, its subcollections, and its items) so that the text annotators read isn't taken out of context (in the [ArchivesSpace](https://archives.collections.ed.ac.uk/) catalog, metadata descriptions are organized hierarchically with items in subcollections and subcollections in collections).\n",
    "\n",
    "**Note:** Collections are of vastly different sizes and their descriptions of varying lengths, so the amount of text in one collection should not be used as a proxy for all the collections.  The longest collection is Coll-41, The Papers of Conrad Hal Waddington, so that can be used as the maximum text that may appear in a single collection and single annotation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeReadable(f):\n",
    "    # Remove curly braces and empty sets, and add empty\n",
    "    # lines between descriptions and field names\n",
    "    f = f.replace('}', '\\n')\n",
    "    f = f.replace('{', '')\n",
    "    f = f.replace('set()', 'No description provided \\n')\n",
    "\n",
    "    # Add space after 'Fonds ID:'\n",
    "    f.replace('Fonds ID:', 'Fonds ID: ')\n",
    "\n",
    "    # Replace metadata field names with their\n",
    "    # corresponding headings on ArchivesSpace\n",
    "    f = f.replace('unitids', 'Collection, Sub-collection, and Item IDs')\n",
    "    f = f.replace('scopecontent', 'Scope and Contents')\n",
    "    f = f.replace('bioghist', 'Biographical / Historical')\n",
    "    f = f.replace('processinfo', 'Processing Information')\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      " Fonds ID:Coll-1149\n",
      "Collection, Sub-collection, and Item IDs\n",
      "'Coll-1149'\n",
      "\n",
      "Scope and Contents\n",
      "'This key research resource is an important survival, being a manuscript account book detailing transactions - debits and credits - relating to the lead-ore company at Leadhills, operated by Sir John Hope of Craighall. Many important people are mentioned in this book, including Alexander Hope of London, Archibald Hope of Craighall, the Earl of Wigtown, the Duke of Hamilton, the Lord of Inglestone, Charles\n",
      "2\n",
      " nner - 'Empress of Britain' - Sunday 27 August - A crofters cottage on the island of Harris / Sir Alexander Mackenzie 1 x menu card - with abstract of log - diner au revoir - 'Empress of Britain' - Monday 28 August - Westerham / Maj-Gen. James Wolfe\"\n",
      "\n",
      "Biographical / Historical\n",
      "\"For the Canadian Pacific Steamships Ltd., Atlantic passenger carrying would last barely four decades from 1921. In the 1960s when air travel and cargo containerisation started to compete with North Atlantic shipping compa\n",
      "3\n",
      " ne, on Deuteronomy, Proverbs, Jeremiah, Schweich lectures, Semitica, Old Testament notes, travels in Syria, covering the period 1891-1923; material relating to Frank Scholten's Palestine, including correspondence and 23 mounted photographs; photograph albums of Palestine, 1891 and 1901, travels in Europe but mainly Switzerland, 1883-1886, and Egypt; and, various printed publications including The deliverance of Palestine (1927), reprint of Israel by Julius Wellhausen, copies of parts of the Hebr\n",
      "4\n",
      " respondence of Mrs Crudelius 1869 - 1870', \"The subject of the volume is 'a history of the Ladies' Edinburgh Debating Society 1865 - 1935'. Wrapped in a envelope with 'Copy belonging to Miss Alice Smith, presented to the Association by Miss Ailie Sempiel her niece.' written on it.\", 'Register of certificates in arts of the Edinburgh Association for the University Education of Women 1876 - 1893. Indexed. List of the students who received certificates in the arts, along with their addresses, the s\n"
     ]
    }
   ],
   "source": [
    "dataset1 = makeReadable(dataset1)\n",
    "dataset2 = makeReadable(dataset2)\n",
    "dataset3 = makeReadable(dataset3)\n",
    "dataset4 = makeReadable(dataset4)\n",
    "\n",
    "print(\"1\\n\",dataset1[:500])\n",
    "print(\"2\\n\",dataset2[500:1000])\n",
    "print(\"3\\n\",dataset3[1000:1500])\n",
    "print(\"4\\n\",dataset4[1500:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the string in each dataset file into several strings, one for each fonds (collection) ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = dataset1.split('Fonds ')[1:]\n",
    "dataset2 = dataset2.split('Fonds ')[1:]\n",
    "dataset3 = dataset3.split('Fonds ')[1:]\n",
    "dataset4 = dataset4.split('Fonds ')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset1\n",
    "#dataset2\n",
    "#dataset3\n",
    "# dataset4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:Coll-1149\n",
      "Collection, Sub-collection, and Item IDs\n",
      "'Coll-1149'\n",
      "\n",
      "Scope and Contents\n",
      "'This key research resource is an important survival, being a manuscript account book detailing transactions - debits and credits - relating to the lead-ore company at Leadhills, operated by Sir John Hope of Craighall. Many important people are mentioned in this book, including Alexander Hope of London, Archibald Hope of Craighall, the Earl of Wigtown, the Duke of Hamilton, the Lord of Inglestone, Charles Erskine of Alba, Alexander Tait, Lady Marie Keith, the Earl of Crawford, Lord Mordington, Lord Cardcross, and Alexander Ross. The amounts involved are huge, with the account of revenues in hand running to over £70,000 towards the end of the period. The manuscript volume itself is composed of a short alphabetic table of names, then from folio 1, accounts dating from 1 August 1662, Edinburgh, to 7 September 1671, Edinburgh, at folio 221. Towards the rear of the volume are another set of accounts and revenues and interests on 87 folios.'\n",
      "\n",
      "Biographical / Historical\n",
      "\"The Scottish judge, Sir John Hope, Lord Craighall, was born around 1605 and was the eldest son of Sir Thomas Hope of Craighall, first baronet. He studied for the legal profession, became an advocate, and was knighted in 1632. In 1640 he was placed on the Committee of Estates appointed to provide for the defence of the country against King Charles I (the Committee of Estates had been appointed to govern Scotland when Parliament was not in session). In 1651, his brother Sir Alexander Hope was examined by the Committee of Estates for advising the King to surrender Scotland and Ireland to Cromwell, during which he quoted Sir John, Lord Craighall, as having stated that the King should 'treat with Cromwell for one-half of his coat before he lost the whole'. In 1652, Sir John was appointed as one of three Scottish judges who, together with five English judges, comprised Cromwell's committee responsible for the administration of justice. Another brother, Sir James Hope (1614-1661), was a representative for Scotland in the English Parliament in 1653. Sir John Hope, Lord Craighall, died in Edinburgh on 28 April 1654. It had been Sir John Hope and Sir James Hope who in the 1660s had owned the lead-ore company at Leadhills in the Lowther Hills - an area along with Wanlockhead which yielded other ores too. Indeed gold was discovered in the area during the reign of James IV, when gold-mining had provided employment for 300 persons. Although gold-mining declined, the export of lead-ore from Leadhills was very important in the early modern Scottish economy, and the Hope family had built on their control of this industry to acquire property and land. The lead mines were profitable enough to justify building or improving roads the full 50 miles to Leith from which the ore could be exported to the Low Countries for smelting.\"\n",
      "\n",
      "Processing Information\n",
      "No description provided \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks as expected!\n",
    "\n",
    "Next, write each collection's descriptions to a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Data Files: 592\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "####################\n",
    "datasets = [dataset1, dataset2, dataset3]\n",
    "fileCount = 0\n",
    "i = 1                                             # identifier for subset of training data\n",
    "for d in datasets:\n",
    "    j = 0                                         # index of collection string in subset of training data\n",
    "    for coll in d:\n",
    "        filename = 'training' + str(i) + '-'+ str(j) + '.txt'\n",
    "        filepath = 'bratTxts/' + filename\n",
    "        f = open(filepath, 'x')\n",
    "        f.write(coll)\n",
    "        f.close()\n",
    "        j += 1\n",
    "        fileCount += 1\n",
    "    i += 1\n",
    "print(\"Total Training Data Files:\",fileCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Development Data Files: 197\n"
     ]
    }
   ],
   "source": [
    "# Development Data\n",
    "####################\n",
    "j = 0                                         # index of collection string in subset of development data\n",
    "for coll in dataset4:\n",
    "    filename = 'dev' + str(j) + '.txt'\n",
    "    filepath = 'bratTxts/' + filename\n",
    "    f = open(filepath, 'x')\n",
    "    f.write(coll)\n",
    "    f.close()\n",
    "    j += 1\n",
    "print(\"Total Development Data Files:\",j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total brat files: 789\n"
     ]
    }
   ],
   "source": [
    "print(\"Total brat files:\",fileCount+j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the resulting files into brat for annotating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics of Descriptions to be Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [dataset1, dataset2, dataset3, dataset4]\n",
    "headings = [\"ID:\", \"Collection, Sub-collection, and Item IDs\", \"Scope and Contents\", \"Biographical / Historical\", \"Processing Information\", \"No description provided\"]\n",
    "descs = []\n",
    "for data in datasets:\n",
    "    for s in data:\n",
    "        coll_ids = re.findall(\"Coll-\\d{4}\", s)\n",
    "        for coll_id in coll_ids:\n",
    "            s = s.replace(coll_id, \"\")\n",
    "        for heading in headings:\n",
    "            s = s.replace(heading, \"\")\n",
    "        s = s.strip()\n",
    "        descs += [s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "''\n",
      "\n",
      "\n",
      "\"The manuscript materialLe Thresor des Divines et Celestes Consolations'(London, 1643) was bound by 'Lord Herbert's Binder'. It contains 21 chapters on the nature and benefits of Afflictions. A rough translation of the introduction gives the flavour: 'Friendly Reader, this book, to which I have given light, shows how tribulations tear us away from sin, which is the source and origin of all pain; it brings us to virtue, to good, and to God, who is the means, the Principle, indeed who is in Himself all the Sovereign good. And afterwards it produces the means to keep always on the right path of virtue, eases our path towards Heaven, and forces us through a secret violence and voluntary constraints, despising that which is of the world (holding its voluptuousness, its delights and vanities against one's will and in disgust) and to breathe towards Heaven, with tears in the eyes, sighs on the lips and sobs in the heart.' The volume is dedicated to Edward Montagu, 2nd. Earl of Manchester (1602-1671), the Presbyterian and a Parliamentary leader. Manchester had succeeded his father as 2nd. Earl of Manchester in November 1642. In August 1643 he was appointed Sergeant-Major-General of the Associated Counties of East Anglia.\"\n",
      "\n",
      "\n",
      "\"The name David de Hasteville, in London, first appears in theCalendar of State Papers, Domestic series, of the reign of Charles I. 1640-41., p.377, in the 'Petition of David de Hasteville, formerly called Father Archange de Hasteville, Abbé du Val de Sainte Croix and General of the Order of St. Romuald to [Sec. Windebank]'. De Hasteville had recently become a Protestant and was seeking 'some sort of entertainment or pension'. He had also petitioned the House of Commons in 1648, describing himself as a Protestant who had recently left his native France and 'a plentiful estate'. Apparently in 1643 he had been appointed general of artillery under Sir William Waller, and had raised three hundred and fourteen men at his own cost, not receiving any government cash, and was then in danger of arrest for money owed for the billetting of his soldiers.\"\n"
     ]
    }
   ],
   "source": [
    "print(type(descs[1]))\n",
    "print(descs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789\n"
     ]
    }
   ],
   "source": [
    "fileCount = 0\n",
    "i = 0   # identifier for subset of data\n",
    "for desc in descs:\n",
    "    filename = 'desc' + str(i) + '.txt'\n",
    "    filepath = 'DatasetExports/DescriptionsOnly/' + filename\n",
    "    f = open(filepath, 'x')\n",
    "    f.write(desc)\n",
    "    f.close()\n",
    "    fileCount += 1\n",
    "    i += 1\n",
    "print(fileCount) # File count should be 789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlists = PlaintextCorpusReader(\"DatasetExports/DescriptionsOnly/\", '\\w+\\d{1}.txt', encoding='utf-8')\n",
    "fileids = wordlists.fileids()\n",
    "tokens = wordlists.words()\n",
    "# tokens[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(wordlists.raw())\n",
    "# sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 77093\n",
      "Total words: 1279713\n"
     ]
    }
   ],
   "source": [
    "print(\"Total sentences:\",len(sentences))\n",
    "alpha_tokens = [t for t in tokens if t.isalpha()]\n",
    "print(\"Total words:\",len(alpha_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentences per file: 97.70975918884665\n"
     ]
    }
   ],
   "source": [
    "print(\"Average sentences per file:\", len(sentences)/fileCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# https://www.wikiwand.com/en/Courtesy_titles_in_the_United_Kingdom#/Scottish_courtesy_titles\n",
    "# https://en.wikipedia.org/wiki/English_honorifics\n",
    "\n",
    "fem_titles = [\"Madam\", \"Madame\", \"Ma'am\", \"Lady\", \"Queen\", \"Dame\", \"Duchess\", \"Miss\", \"Ms\", \"Mrs\", \"Missus\", \"Mx\", \"Marchioness\", \"Countess\", \"Viscountess\", \"Baroness\", \"Maid\"]\n",
    "masc_titles = [\"Sir\", \"Lord\", \"King\", \"Duke\", \"Mr\", \"Sire\", \"Gentleman\", \"Marquess\", \"Viscount\", \"Baron\", \"Laird\"]\n",
    "fem_pronouns = [\"she\", \"her\"]\n",
    "masc_pronouns = [\"him\", \"his\", \"he\"]\n",
    "both_pronouns = [\"they\", \"their\", \"them\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem_tokens = [t for t in alpha_tokens if t in fem_titles]\n",
    "fem_pronouns = [t for t in alpha_tokens if t in fem_pronouns]\n",
    "masc_pronouns = [t for t in alpha_tokens if t in masc_pronouns]\n",
    "masc_tokens = [t for t in alpha_tokens if t in masc_titles]\n",
    "both_tokens = [t for t in alpha_tokens if t in both_pronouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feminine Titles: 1631 (28.887708111937656%)\n",
      "Feminine Pronouns: 4131 (25.792957042957042%)\n",
      "Masculine Titles: 4015 (71.11229188806234%)\n",
      "Masculine Pronouns: 11882 (74.18831168831169%)\n",
      "Both Pronouns: 3 (0.018731268731268732%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Feminine Titles:\", len(fem_tokens), \"(\"+str((len(fem_tokens)/(len(masc_tokens)+len(fem_tokens)))*100)+\"%)\")\n",
    "print(\"Feminine Pronouns:\", len(fem_pronouns), \"(\"+str((len(fem_pronouns)/(len(masc_pronouns)+len(fem_pronouns)+len(both_pronouns)))*100)+\"%)\")\n",
    "print(\"Masculine Titles:\", len(masc_tokens), \"(\"+str((len(masc_tokens)/(len(masc_tokens)+len(fem_tokens)))*100)+\"%)\")\n",
    "print(\"Masculine Pronouns:\", len(masc_pronouns), \"(\"+str((len(masc_pronouns)/(len(masc_pronouns)+len(fem_pronouns)+len(both_pronouns)))*100)+\"%)\")\n",
    "print(\"Both Pronouns:\", len(both_pronouns), \"(\"+str((len(both_pronouns)/(len(masc_pronouns)+len(fem_pronouns)+len(both_pronouns)))*100)+\"%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Time Needed for Annotation\n",
    "Pilot 1 (myself):\n",
    "* Files visited: training1-0.txt - training1-121.txt\n",
    "* Files annotated: 26 out of 27 files\n",
    "* Total time: 1 hour, 30 minutes\n",
    "\n",
    "Pilot 2 (three participants):\n",
    "* Files visited: training1-122.txt - training1-130.txt (10); training2-0.txt - training2-103.txt (7); training3-0.txt - training3-108.txt (11)\n",
    "* Files annotated: 10 out of 10 files; 6 out of 7 files; 10 out of 11 files\n",
    "* Total time: 30 minutes\n",
    "\n",
    "Estimates by file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time for me to annotate: 54.77777777777778\n",
      "Estimated total weeks at 9 hours per week: 6.08641975308642\n"
     ]
    }
   ],
   "source": [
    "total_files = 789 + 197\n",
    "est_total_hrs = (total_files/27)*1.5\n",
    "print(\"Estimated time for me to annotate:\",est_total_hrs)\n",
    "print(\"Estimated total weeks at 9 hours per week:\", est_total_hrs/9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum estimated time to annotate: 70.42857142857143\n",
      "Maximum estimated weeks at 9 hours per week: 7.825396825396826\n"
     ]
    }
   ],
   "source": [
    "est2_total_hrs = (total_files/7)*0.5\n",
    "print(\"Maximum estimated time to annotate:\", est2_total_hrs)\n",
    "print(\"Maximum estimated weeks at 9 hours per week:\",est2_total_hrs/9.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimates by token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlists = PlaintextCorpusReader(\"bratTxts/\", '.+(.txt)', encoding='utf-8')\n",
    "fileids = wordlists.fileids()\n",
    "# print(fileids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2082296\n"
     ]
    }
   ],
   "source": [
    "tokens = wordlists.words()\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot1 = [\"training1-0.txt\", \"training1-1.txt\", \"training1-10.txt\", \"training1-100.txt\", \"training1-101.txt\", \"training1-102.txt\", \n",
    "          \"training1-103.txt\", \"training1-104.txt\", \"training1-105.txt\", \"training1-106.txt\", \"training1-107.txt\", \"training1-108.txt\",\n",
    "          \"training1-109.txt\", \"training1-11.txt\", \"training1-110.txt\", \"training1-111.txt\", \"training1-112.txt\", \"training1-113.txt\",\n",
    "          \"training1-114.txt\", \"training1-115.txt\", \"training1-116.txt\", \"training1-117.txt\", \"training1-118.txt\", \"training1-119.txt\",\n",
    "          \"training1-12.txt\", \"training1-120.txt\", \"training1-121.txt\"\n",
    "         ]\n",
    "pilot2a = [\"training1-122.txt\", \"training1-123.txt\", \"training1-124.txt\", \"training1-125.txt\", \"training1-126.txt\", \"training1-127.txt\",\n",
    "          \"training1-128.txt\", \"training1-129.txt\", \"training1-13.txt\", \"training1-130.txt\"]\n",
    "pilot2b = [\"training2-0.txt\", \"training2-1.txt\", \"training2-10.txt\", \"training2-100.txt\", \"training2-101.txt\", \"training2-102.txt\",\n",
    "          \"training2-103.txt\"\n",
    "          ]\n",
    "pilot2c = [\"training3-0.txt\", \"training3-1.txt\", \"training3-10.txt\", \"training3-100.txt\", \"training3-101.txt\", \"training3-102.txt\",\n",
    "          \"training3-103.txt\", \"training3-104.txt\", \"training3-105.txt\", \"training3-106.txt\", \"training3-107.txt\", \"training3-108.txt\"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pilot 1 Total Tokens: 13419\n",
      "Tokens per hour: 8946.0\n",
      "Total hours: 232.76279901632014\n",
      "Total weeks at 20 hours per week: 11.638139950816008\n"
     ]
    }
   ],
   "source": [
    "pilot1_tokens = 0\n",
    "for filename in pilot1:\n",
    "    pilot1_tokens += len(wordlists.words(filename))\n",
    "print(\"Pilot 1 Total Tokens:\", pilot1_tokens)\n",
    "print(\"Tokens per hour:\", ((pilot1_tokens/3)*2))\n",
    "print(\"Total hours:\", (len(tokens))/((pilot1_tokens/3)*2))\n",
    "print(\"Total weeks at 20 hours per week:\", (len(tokens))/((pilot1_tokens/3)*2)/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pilot 2a Total Tokens: 3977\n",
      "Tokens per hour: 2651.3333333333335\n",
      "Total hours: 785.3769172743273\n",
      "Total weeks at 9 hours per week: 87.2641019193697\n",
      "Total tokens in 8 weeks at 9 hours per week: 190896.0  ( 0.09167572717807651 % of data)\n"
     ]
    }
   ],
   "source": [
    "pilot2a_tokens = 0\n",
    "for filename in pilot2a:\n",
    "    pilot2a_tokens += len(wordlists.words(filename))\n",
    "print(\"Pilot 2a Total Tokens:\", pilot2a_tokens)\n",
    "print(\"Tokens per hour:\", ((pilot2a_tokens/3)*2))\n",
    "print(\"Total hours:\", (len(tokens))/((pilot2a_tokens/3)*2))\n",
    "print(\"Total weeks at 9 hours per week:\", (len(tokens))/((pilot2a_tokens/3)*2)/9)\n",
    "print(\"Total tokens in 8 weeks at 9 hours per week:\", ((pilot2a_tokens/3)*2)*(9*8), \" (\", ((pilot2a_tokens/3)*2)*(9*8)/(len(tokens)), \"% of data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pilot 2b Total Tokens: 3465\n",
      "Tokens per hour: 2310.0\n",
      "Total hours: 901.4268398268398\n",
      "Total weeks at 9 hours per week: 100.15853775853776\n",
      "***Note that this pilot annotator was a non-native English speaker!\n"
     ]
    }
   ],
   "source": [
    "pilot2b_tokens = 0\n",
    "for filename in pilot2b:\n",
    "    pilot2b_tokens += len(wordlists.words(filename))\n",
    "print(\"Pilot 2b Total Tokens:\", pilot2b_tokens)\n",
    "print(\"Tokens per hour:\", ((pilot2b_tokens/3)*2))\n",
    "print(\"Total hours:\", (len(tokens))/((pilot2b_tokens/3)*2))\n",
    "print(\"Total weeks at 9 hours per week:\", (len(tokens))/((pilot2b_tokens/3)*2)/9)\n",
    "print(\"***Note that this pilot annotator was a non-native English speaker!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pilot 2b Total Tokens: 5387\n",
      "Tokens per hour: 3591.3333333333335\n",
      "Total hours: 579.8113978095415\n",
      "Total weeks at 9 hours per week: 64.42348864550462\n",
      "Total weeks at 9 hours per week: 64.42348864550462\n",
      "Total tokens in 8 weeks at 9 hours per week: 258576.0  ( 0.12417831086454567 % of data)\n"
     ]
    }
   ],
   "source": [
    "pilot2c_tokens = 0\n",
    "for filename in pilot2c:\n",
    "    pilot2c_tokens += len(wordlists.words(filename))\n",
    "print(\"Pilot 2b Total Tokens:\", pilot2c_tokens)\n",
    "print(\"Tokens per hour:\", ((pilot2c_tokens/3)*2))\n",
    "print(\"Total hours:\", (len(tokens))/((pilot2c_tokens/3)*2))\n",
    "print(\"Total weeks at 9 hours per week:\", (len(tokens))/((pilot2c_tokens/3)*2)/9)\n",
    "print(\"Total weeks at 9 hours per week:\", (len(tokens))/((pilot2c_tokens/3)*2)/9)\n",
    "print(\"Total tokens in 8 weeks at 9 hours per week:\", ((pilot2c_tokens/3)*2)*(9*8), \" (\", ((pilot2c_tokens/3)*2)*(9*8)/(len(tokens)), \"% of data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Considering I had not implemented keyboard shortcuts, it seems likely that I'll be able to hire an annotator to annotate about 10% of the data (calculated by tokens) in 8 weeks at 9 hours per week.**\n",
    "\n",
    "**I should be able to annotate the data in 11-12 weeks at 20 hours per week.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
