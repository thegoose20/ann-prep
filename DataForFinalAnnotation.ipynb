{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data for Final Annotation\n",
    "Harvesting, transforming, and exporting metadata descriptions for annotation of gendered language in [brat](brat.nlplab.org/).\n",
    "\n",
    "* Creator: Lucy Havens\n",
    "* Date: February 18, 2021\n",
    "* Project: PhD research at the School of Informatics, University of Edinburgh\n",
    "* Data Source: Centre for Research Collections' (CRC) [online archival catalog](https://archives.collections.ed.ac.uk/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Harvesting\n",
    "Obtain metadata from the CRC's online archival catalog using the Open Archives Initiative - Protocol for Metadata Harvesting (OAI-PMH).  The CRC provides its metadata in Encoded Archival Description (EAD) format as XML data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for harvesting\n",
    "import xml.dom.minidom\n",
    "import urllib.request\n",
    "import urllib\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element {http://www.openarchives.org/OAI/2.0/}OAI-PMH at 0x7fda159e2440>\n"
     ]
    }
   ],
   "source": [
    "archiveMetadataUrl = \"http://lac-archives-live.is.ed.ac.uk:8082/?verb=ListRecords&metadataPrefix=oai_ead\"\n",
    "\n",
    "def getRootFromUrl(url):\n",
    "    content = urllib.request.urlopen(url)\n",
    "\n",
    "    #tree = ET.parse(content)\n",
    "    parser = etree.XMLParser(recover=True)  # Use recover to try to fix broken XML\n",
    "    tree = etree.parse(content, parser)\n",
    "    \n",
    "    root = tree.getroot()\n",
    "    return root\n",
    "\n",
    "root = getRootFromUrl(archiveMetadataUrl)\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: part of or the entirety of a tag name below which you want to get text \n",
    "# Output: a list of text between tags contained within the inputted tagName, \n",
    "#         with one list element per tagName instance\n",
    "def getTextBeneathTag(root, tagName, header):\n",
    "    text_list = []\n",
    "    for child in root.iter():\n",
    "        tag = child.tag\n",
    "        if tagName in tag:\n",
    "            text_elem = \"\"\n",
    "            for subchild_text in child.itertext():\n",
    "                if header:\n",
    "                    if header not in subchild_text:\n",
    "                        text_elem = text_elem + subchild_text\n",
    "                else:\n",
    "                    text_elem = text_elem + subchild_text\n",
    "            text_list.append(text_elem)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: binary value, url for harvesting metadata, starting prefix for the end of the url, and lists of metadata fields to gather\n",
    "# Output: lists of strings of the gathered metadata fields' descriptions, with one string per fonds, series, and item in the catalog\n",
    "def getDescriptiveMetadata(more, archiveMetadataUrlShort, startingPrefix, ut, ui, ud, gn, lm, sc, bh, pi):    \n",
    "   \n",
    "    archiveMetadataUrlWithPrefix = archiveMetadataUrlShort + startingPrefix\n",
    "    root = getRootFromUrl(archiveMetadataUrlWithPrefix)\n",
    "    ut.append(getTextBeneathTag(root, \"unittitle\", \"Unit Title\"))\n",
    "    ui.append(getTextBeneathTag(root, \"unitid\", \"Unit Identifier\"))\n",
    "    ud.append(getTextBeneathTag(root, \"unitdate\", \"Unit Date\"))\n",
    "    gn.append(getTextBeneathTag(root, \"geogname\", \"Geography Name\"))\n",
    "    lm.append(getTextBeneathTag(root, \"langmaterial\", \"Language of Materials\"))\n",
    "    sc.append(getTextBeneathTag(root, \"scopecontent\", \"Scope and Contents\"))\n",
    "    bh.append(getTextBeneathTag(root, \"bioghist\", \"Biographical / Historical\"))\n",
    "    pi.append(getTextBeneathTag(root, \"processinfo\", \"Processing Information\"))\n",
    "    resumptionToken = getTextBeneathTag(root, \"resumptionToken\", \"\")\n",
    "    \n",
    "    if len(resumptionToken) == 0:\n",
    "        more = False\n",
    "    i = 1\n",
    "    \n",
    "    while more:\n",
    "        archiveMetadataUrlWithToken = archiveMetadataUrlShort + \"resumptionToken=\" + resumptionToken[0]\n",
    "        root = getRootFromUrl(archiveMetadataUrlWithToken)\n",
    "        ut.append(getTextBeneathTag(root, \"unittitle\", \"Unit Title\"))\n",
    "        ui.append(getTextBeneathTag(root, \"unitid\", \"Unit Identifier\"))\n",
    "        ud.append(getTextBeneathTag(root, \"unitdate\", \"Unit Date\"))\n",
    "        gn.append(getTextBeneathTag(root, \"geogname\", \"Geography Name\"))\n",
    "        lm.append(getTextBeneathTag(root, \"langmaterial\", \"Language of Materials\"))\n",
    "        sc.append(getTextBeneathTag(root, \"scopecontent\", \"Scope and Contents\"))\n",
    "        bh.append(getTextBeneathTag(root, \"bioghist\", \"Biographical / Historical\"))\n",
    "        pi.append(getTextBeneathTag(root, \"processinfo\", \"Processing Information\"))\n",
    "        resumptionToken = getTextBeneathTag(root, \"resumptionToken\", \"\")\n",
    "        if len(resumptionToken) == 0:\n",
    "            more = False\n",
    "        i += 1\n",
    "    \n",
    "    print(str(i) + \" resumption tokens\")\n",
    "    return ut, ui, ud, gn, lm, sc, bh, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081 resumption tokens\n"
     ]
    }
   ],
   "source": [
    "url = \"http://lac-archives-live.is.ed.ac.uk:8082/?verb=ListRecords&\"\n",
    "startPrefix = \"metadataPrefix=oai_ead\"\n",
    "ut = [] # List of fonds, series, and item titles\n",
    "ui = [] # List of fonds, series, and item identifiers\n",
    "ud = [] # List of fonds, series, and item dates\n",
    "gn = [] # List of fonds, series, and item associated geographic locations \n",
    "lm = [] # List of fonds, series, and item material languages\n",
    "sc = [] # List of fonds, series, and item \"Scope and Contents\" descriptions\n",
    "bh = [] # List of fonds, series, and item \"Biographical / Historical\" descriptions\n",
    "pi = []  # List of fonds, series, and item \"Processing Information\" descriptions\n",
    "\n",
    "ut, ui, ud, gn, lm, sc, bh, pi = getDescriptiveMetadata(True, url, startPrefix, ut, ui, ud, gn, lm, sc, bh, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081\n",
      "1081\n",
      "1081\n",
      "1081\n",
      "1081\n",
      "1081\n",
      "1081\n",
      "1081\n"
     ]
    }
   ],
   "source": [
    "print(len(ut))\n",
    "print(len(ui))\n",
    "print(len(ud))\n",
    "print(len(gn))\n",
    "print(len(lm))\n",
    "print(len(sc))\n",
    "print(len(bh))\n",
    "print(len(pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n",
      "124\n",
      "124\n",
      "116\n",
      "125\n",
      "119\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(len(ut[i]))\n",
    "print(len(ui[i]))\n",
    "print(len(ud[i]))\n",
    "print(len(gn[i]))\n",
    "print(len(lm[i]))\n",
    "print(len(sc[i]))\n",
    "print(len(bh[i]))\n",
    "print(len(pi[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Transforming\n",
    "Create a table (pandas DataFrame) of the metadata without multi-sentence descriptions and plain text files of the descriptive metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_title</th>\n",
       "      <th>unit_identifier</th>\n",
       "      <th>unit_date</th>\n",
       "      <th>geography</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Papers of Professor Walter Ledermann, 1 (37),...</td>\n",
       "      <td>[Coll-1064, Coll-1064/1, Coll-1064/2, Coll-106...</td>\n",
       "      <td>[1937-1954, 2 Feb 1937, 10 Feb 1937, 16 Feb 19...</td>\n",
       "      <td>[Edinburgh (Scotland), Edinburgh (Scotland), E...</td>\n",
       "      <td>[\\n      English\\n    , English, English, Engl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Drawings from the Office of Sir Rowand Anders...</td>\n",
       "      <td>[Coll-31, Coll-31/1, Coll-31/1/1, Coll-31/1/1/...</td>\n",
       "      <td>[1814-1924, 1874-1905, 1874-1879, 1874-1875, 1...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[\\n      English\\n    , English, English, Engl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Papers of Sir Roderick Impey Murchison and hi...</td>\n",
       "      <td>[Coll-51, Coll-51/1, Coll-51/2, Coll-51/2/1, C...</td>\n",
       "      <td>[1771-1935, 1723-1935, 1770-1938, 1770-1938, 1...</td>\n",
       "      <td>[Calcutta (India), Europe, Scotland, Tarradale...</td>\n",
       "      <td>[\\n      English\\n    , English, English, Engl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Lecture Notes of John Robison, Introductions,...</td>\n",
       "      <td>[Coll-204, Coll-204/1, Coll-204/2, Coll-204/3,...</td>\n",
       "      <td>[c1779-c1801, c1779-c1801, c1804, c1802, c1780...</td>\n",
       "      <td>[Edinburgh (Scotland), Glasgow Lanarkshire Sco...</td>\n",
       "      <td>[\\n      English\\n    , English., English Lati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Records of the Wernerian Natural History Soci...</td>\n",
       "      <td>[Coll-206, Coll-206/1, Coll-206/1/1, Coll-206/...</td>\n",
       "      <td>[1808-1858, 12 January 1808-16 April 1858, 12 ...</td>\n",
       "      <td>[Edinburgh (Scotland), Freiburg im Breisgau (G...</td>\n",
       "      <td>[\\n      English\\n    , English, English, Engl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          unit_title  \\\n",
       "0  [Papers of Professor Walter Ledermann, 1 (37),...   \n",
       "1  [Drawings from the Office of Sir Rowand Anders...   \n",
       "2  [Papers of Sir Roderick Impey Murchison and hi...   \n",
       "3  [Lecture Notes of John Robison, Introductions,...   \n",
       "4  [Records of the Wernerian Natural History Soci...   \n",
       "\n",
       "                                     unit_identifier  \\\n",
       "0  [Coll-1064, Coll-1064/1, Coll-1064/2, Coll-106...   \n",
       "1  [Coll-31, Coll-31/1, Coll-31/1/1, Coll-31/1/1/...   \n",
       "2  [Coll-51, Coll-51/1, Coll-51/2, Coll-51/2/1, C...   \n",
       "3  [Coll-204, Coll-204/1, Coll-204/2, Coll-204/3,...   \n",
       "4  [Coll-206, Coll-206/1, Coll-206/1/1, Coll-206/...   \n",
       "\n",
       "                                           unit_date  \\\n",
       "0  [1937-1954, 2 Feb 1937, 10 Feb 1937, 16 Feb 19...   \n",
       "1  [1814-1924, 1874-1905, 1874-1879, 1874-1875, 1...   \n",
       "2  [1771-1935, 1723-1935, 1770-1938, 1770-1938, 1...   \n",
       "3  [c1779-c1801, c1779-c1801, c1804, c1802, c1780...   \n",
       "4  [1808-1858, 12 January 1808-16 April 1858, 12 ...   \n",
       "\n",
       "                                           geography  \\\n",
       "0  [Edinburgh (Scotland), Edinburgh (Scotland), E...   \n",
       "1                                                 []   \n",
       "2  [Calcutta (India), Europe, Scotland, Tarradale...   \n",
       "3  [Edinburgh (Scotland), Glasgow Lanarkshire Sco...   \n",
       "4  [Edinburgh (Scotland), Freiburg im Breisgau (G...   \n",
       "\n",
       "                                            language  \n",
       "0  [\\n      English\\n    , English, English, Engl...  \n",
       "1  [\\n      English\\n    , English, English, Engl...  \n",
       "2  [\\n      English\\n    , English, English, Engl...  \n",
       "3  [\\n      English\\n    , English., English Lati...  \n",
       "4  [\\n      English\\n    , English, English, Engl...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict({\"unit_title\":ut, \"unit_identifier\":ui, \"unit_date\":ud, \"geography\":gn, \"language\":lm})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"CRC_units-grouped-by-fonds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081\n",
      "['Coll-1064', 'Coll-31', 'Coll-51', 'Coll-204', 'Coll-206', 'Coll-205', 'Coll-1443', 'Coll-1444', 'Coll-1391', 'Coll-1371']\n"
     ]
    }
   ],
   "source": [
    "indeces = []\n",
    "for ui_list in ui:\n",
    "    indeces += [ui_list[0]]\n",
    "print(len(indeces))\n",
    "print(indeces[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def flattenTwoDimensionalList(two_d_list):\n",
    "#     flattened = []\n",
    "#     for listoflists in two_d_list:\n",
    "#         for unit in listoflists:\n",
    "#             flattened += [unit]\n",
    "#     return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# titles = flattenTwoDimensionalList(ut)\n",
    "# # print(titles[0:30])\n",
    "# identifiers = flattenTwoDimensionalList(ui)\n",
    "# dates = flattenTwoDimensionalList(ud)\n",
    "# geogs = flattenTwoDimensionalList(gn)\n",
    "# lang = flattenTwoDimensionalList(lm)\n",
    "# scopecont = flattenTwoDimensionalList(sc)\n",
    "# bioghist = flattenTwoDimensionalList(bh)\n",
    "# procinfo = flattenTwoDimensionalList(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print(len(titles))\n",
    "# print(len(identifiers))\n",
    "# print(len(dates))\n",
    "# print(len(geogs))\n",
    "# print(len(lang))\n",
    "# print(len(scopecont))\n",
    "# print(len(bioghist))\n",
    "# print(len(procinfo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeListsToFilesPerFonds(indeces, titles, scopeconts, bioghists, procinfo):\n",
    "    maxI = len(indeces)\n",
    "    i = 0\n",
    "    while i < maxI:\n",
    "        filename = (indeces[i]).strip()\n",
    "        filename = filename.replace(\" \", \"_\")\n",
    "        filename = filename.replace(\"/\", \"_\")\n",
    "        filepath = \"descriptions_by_fonds/\"+filename+\".txt\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(\"Identifier: \")\n",
    "            f.write(filename + \"\\n\")\n",
    "            for t in titles[i]:\n",
    "                t = t.strip()\n",
    "                f.write(\"\\nTitle:\\n\")\n",
    "                f.write(t + \"\\n\")\n",
    "            for s in scopeconts[i]:\n",
    "                s = s.strip()\n",
    "                f.write(\"\\nScope and Contents:\\n\")\n",
    "                f.write(s + \"\\n\")\n",
    "            for b in bioghists[i]:\n",
    "                b = b.strip()\n",
    "                f.write(\"\\nBiographical / Historical:\\n\")\n",
    "                f.write(b + \"\\n\")\n",
    "            for p in procinfo[i]:\n",
    "                p = p.strip()\n",
    "                f.write(\"\\nProcessing Information:\\n\")\n",
    "                f.write(p + \"\\n\")\n",
    "        f.close()\n",
    "        i += 1\n",
    "    return str(maxI) + \" files written\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1081 files written'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writeListsToFilesPerFonds(indeces, ut, sc, bh, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Preparing\n",
    "Prepare the files for annotation, ensuring ease in reading and splitting any excessively long files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Libraries for Natural Language Processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.text import Text\n",
    "# nltk.download('punkt')\n",
    "from nltk.probability import FreqDist\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'descriptions_by_fonds/'\n",
    "files = PlaintextCorpusReader(directory, '.+')\n",
    "tokens = files.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Identifier', ':', 'AA4', 'Title', ':', 'Papers', 'of', 'Rev', 'Prof', 'John', 'McIntyre', '(', '1916', '-', '2005', ')', 'Scope', 'and', 'Contents', ':']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_totals = []\n",
    "filenames = files.fileids()\n",
    "for f in filenames:\n",
    "        token_totals += [len(files.words(f))]\n",
    "file_lengths = dict(zip(filenames,token_totals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8, 9, 14, 2063, 15, 4115, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 10290, 52, 54, 55, 56, 24632, 57, 59, 58, 61, 53, 63, 64, 65, 66, 67, 68, 60, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 85, 86, 89, 91, 95, 8287, 97, 99, 100, 103, 104, 105, 106, 107, 108, 109, 110, 114, 116, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 129, 131, 2180, 133, 134, 136, 137, 138, 139, 140, 20621, 142, 143, 144, 145, 147, 148, 149, 150, 151, 153, 154, 156, 157, 158, 159, 161, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 2223, 184, 186, 187, 185, 190, 192, 196, 198, 199, 200, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 222, 223, 224, 225, 226, 227, 230, 4327, 231, 233, 234, 235, 236, 237, 45289, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 272, 273, 274, 275, 276, 28948, 277, 279, 280, 282, 283, 284, 286, 287, 288, 289, 290, 291, 292, 294, 295, 2343, 297, 299, 300, 301, 302, 303, 304, 305, 307, 308, 309, 311, 312, 313, 314, 62, 316, 317, 318, 319, 321, 323, 324, 327, 328, 329, 330, 331, 332, 333, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 349, 350, 351, 352, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 369, 371, 372, 374, 375, 377, 631161, 379, 381, 383, 2432, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 397, 399, 402, 403, 405, 406, 407, 409, 410, 411, 413, 415, 417, 419, 420, 421, 423, 427, 428, 429, 430, 431, 432, 433, 434, 435, 12724, 437, 438, 439, 440, 441, 442, 4534, 444, 446, 448, 449, 450, 14787, 4547, 456, 458, 462, 464, 465, 467, 470, 471, 472, 473, 474, 475, 476, 477, 478, 480, 486, 10728, 490, 491, 492, 493, 494, 495, 497, 498, 499, 500, 502, 503, 25078, 505, 507, 508, 510, 512, 513, 8706, 516, 519, 521, 523, 524, 525, 526, 527, 530, 531, 533, 536, 542, 543, 546, 547, 548, 554, 555, 557, 558, 560, 563, 567, 568, 569, 43579, 572, 574, 576, 578, 583, 584, 585, 586, 587, 590, 593, 595, 598, 600, 605, 607, 609, 613, 614, 618, 619, 624, 628, 629, 634, 635, 636, 637, 642, 643, 648, 651, 653, 654, 656, 658, 659, 660, 661, 666, 12956, 671, 675, 676, 677, 678, 680, 682, 684, 689, 690, 696, 698, 709, 712, 713, 716, 724, 725, 733, 740, 742, 744, 756, 6903, 759, 770, 2820, 774, 779, 780, 787, 6934, 797, 803, 815, 821, 826, 828, 832, 833, 835, 837, 838, 839, 851, 859, 861, 9059, 870, 876, 877, 889, 2942, 895, 4995, 916, 918, 926, 927, 2978, 930, 939, 947, 7106, 964, 11226, 1000, 1006, 1008, 1010, 1022, 244739, 1035, 1041, 1054, 7198, 1056, 1059, 3110, 1064, 1066, 1067, 1072, 35895, 1092, 3175, 1131, 11372, 1134, 3190, 1153, 13453, 1168, 1175, 1180, 1183, 1186, 1190, 1191, 1196, 1209, 1222, 1223, 3286, 1242, 36059, 3302, 3315, 216326, 11542, 1303, 1309, 1341, 87369, 3415, 1367, 396645, 1393, 1408, 3462, 1415, 23943, 1482, 1485, 1488, 1501, 7660, 1519, 1527, 3594, 1559, 1603, 1614, 28245, 1632, 20066, 1653, 1654, 13966, 16038, 1706, 1721, 147138, 7875, 32495, 3829, 16139, 98115, 5990, 1947, 63458, 6114}\n"
     ]
    }
   ],
   "source": [
    "token_totals.sort\n",
    "print(set(token_totals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "826"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_lengths[\"Coll-1250.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "too_long = []\n",
    "for key,value in file_lengths.items():\n",
    "    if value > 1000:\n",
    "        too_long += [key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BAI.txt', 'Coll-1022.txt', 'Coll-1036.txt', 'Coll-1052.txt', 'Coll-1057.txt', 'Coll-1059.txt', 'Coll-1060.txt', 'Coll-1061.txt', 'Coll-1062.txt', 'Coll-1064.txt', 'Coll-1066.txt', 'Coll-1142.txt', 'Coll-1146.txt', 'Coll-1156.txt', 'Coll-1162.txt', 'Coll-1167.txt', 'Coll-1242.txt', 'Coll-1243.txt', 'Coll-1247.txt', 'Coll-1255.txt', 'Coll-1257.txt', 'Coll-1260.txt', 'Coll-1266.txt', 'Coll-1294.txt', 'Coll-13.txt', 'Coll-1310.txt', 'Coll-1320.txt', 'Coll-1329.txt', 'Coll-1357.txt', 'Coll-1362.txt', 'Coll-1363.txt', 'Coll-1364.txt', 'Coll-1373.txt', 'Coll-1383.txt', 'Coll-1385.txt', 'Coll-14.txt', 'Coll-1434.txt', 'Coll-1443.txt', 'Coll-146.txt', 'Coll-1461.txt', 'Coll-1489.txt', 'Coll-1490.txt', 'Coll-1492.txt', 'Coll-1496.txt', 'Coll-1497.txt', 'Coll-1499.txt', 'Coll-1527.txt', 'Coll-1528.txt', 'Coll-1541.txt', 'Coll-1549.txt', 'Coll-1557.txt', 'Coll-1574.txt', 'Coll-1577.txt', 'Coll-1580.txt', 'Coll-1583.txt', 'Coll-1586.txt', 'Coll-1593.txt', 'Coll-16.txt', 'Coll-1613.txt', 'Coll-1623.txt', 'Coll-1638.txt', 'Coll-164.txt', 'Coll-1644.txt', 'Coll-1650.txt', 'Coll-1651.txt', 'Coll-1657.txt', 'Coll-1671.txt', 'Coll-1690.txt', 'Coll-17.txt', 'Coll-1700.txt', 'Coll-1711.txt', 'Coll-1715.txt', 'Coll-1725.txt', 'Coll-1728.txt', 'Coll-1749.txt', 'Coll-1756.txt', 'Coll-1758.txt', 'Coll-1798.txt', 'Coll-203.txt', 'Coll-204.txt', 'Coll-205.txt', 'Coll-206.txt', 'Coll-21.txt', 'Coll-27.txt', 'Coll-31.txt', 'Coll-36.txt', 'Coll-39.txt', 'Coll-41.txt', 'Coll-411.txt', 'Coll-42.txt', 'Coll-43.txt', 'Coll-51.txt', 'Coll-54.txt', 'Coll-64.txt', 'Coll-66.txt', 'Coll-67.txt', 'Coll-68.txt', 'Coll-70.txt', 'Coll-706.txt', 'Coll-71.txt', 'Coll-74.txt', 'Coll-85.txt', 'Coll-99.txt', 'ECA_TYP.txt', 'EUA_CA1.txt', 'EUA_CA14.txt', 'EUA_CA2.txt', 'EUA_GD31.txt', 'EUA_GD46.txt', 'EUA_GD58.txt', 'EUA_GD9.txt', 'EUA_IN1.txt', 'EUA_IN2.txt', 'EUA_IN20.txt', 'EUA_IN23.txt', 'GD2.txt', 'La.txt', 'Or_Ms.txt', 'PJM.txt']\n",
      "119\n"
     ]
    }
   ],
   "source": [
    "print(too_long)\n",
    "print(len(too_long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of files to break up manually, so let's use Python to divide these large files into smaller files with a maximum of 100 lines each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code in this cell from:\n",
    "# https://stackoverflow.com/questions/16289859/splitting-large-text-file-into-smaller-text-files-by-line-numbers-using-python\n",
    "def splitLargeFile(f, max_lines, old_dir, new_dir):\n",
    "    short = None\n",
    "    file_path = old_dir+f\n",
    "    with open(file_path) as long:\n",
    "        for line_no, line in enumerate(long):\n",
    "            if line_no % max_lines == 0:\n",
    "                if short:\n",
    "                    short.close()\n",
    "                f = f.replace(\".txt\",\"_\")\n",
    "                short_name = str(f)+'{}.txt'.format(line_no + max_lines)\n",
    "                new_path = new_dir+short_name\n",
    "                short = open(new_path, \"w\")\n",
    "            short.write(line)\n",
    "        if short:\n",
    "            short.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in filenames:\n",
    "    splitLargeFile(f, 100, \"descriptions_by_fonds/\", \"for_brat/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
